import os
import json
from shutil import rmtree
import logging
import ffmpeg
from datetime import datetime, timedelta
from airflow.decorators import dag, task, task_group
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.postgres.hooks.postgres import PostgresHook
from airflow.operators.empty import EmptyOperator
from airflow.exceptions import AirflowException, AirflowSkipException, AirflowFailException
from airflow.models import Variable
from airflow.hooks.base import BaseHook

s3_bucket = Variable.get("S3_BUCKET")
postgres_hook = PostgresHook(postgres_conn_id="postgres_conn")
s3_hook = S3Hook(aws_conn_id="aws_conn")

get_course_records_sql = """
SELECT course_id, video_url, raw_video_url
FROM schema_name.courses
WHERE 1=1
  and (raw_video_url ilike '%.mp4')
  and (video_url ilike '%.mp4' or video_url is null)
ORDER BY course_id desc
"""

update_course_video_sql = """
UPDATE schema_name.courses
SET
  video_url = '{s3_m3u8_path}',
  duration_seconds = {duration_seconds},
  processing_completion_time = '{timestamp}'
WHERE course_id = {course_id} and raw_video_url = '{raw_video_url}'
"""


default_args = {
  "owner": "Julie Scherer",
  "retries": 2,
  "retry_delay": timedelta(minutes=2),
  "execution_timeout": timedelta(hours=3),
}
@dag(
  start_date=datetime(2024, 5, 1),
  schedule_interval=timedelta(hours=3),
  catchup=False,
  default_args=default_args,
)
def video_processing_dag_v2():

  ## Task #1 
  start = EmptyOperator(task_id="start")
  
  ## Task #2
  @task
  def get_mapped_courses() -> list[dict]:
    query = get_course_records_sql
    logging.info(f"Query to execute: {query}")
    records = postgres_hook.get_records(sql=query)
    if not records:
      raise AirflowSkipException(f"No records found. Return value: {records}") # Skip if there are no records
    
    mapped_courses = []
    for course_id, video_url, raw_video_url in records:
      mp4_file = os.path.basename(raw_video_url) #.replace("__", "_").replace(" ", "_").replace("-", "_").replace("/", "_").replace("\\", "_").replace(",", "_").replace("..",".").replace("__", "_").lower()
      local_mp4_dir = f"include/datasets/videos/{course_id}"
      local_mp4_path = os.path.join(local_mp4_dir, mp4_file)
      base_filename = os.path.splitext(os.path.basename(local_mp4_path))[0].replace(" ", "_").lower()
      local_m3u8_dir = os.path.join(os.path.dirname(local_mp4_path), base_filename)
      s3_m3u8_dir = os.path.join(os.path.dirname(raw_video_url), base_filename)
      params = {
        "s3_bucket": s3_bucket,
        "video_url": video_url,
        "raw_video_url": raw_video_url,
        "mp4_file": mp4_file,
        "local_mp4_dir": local_mp4_dir,
        "local_mp4_path": local_mp4_path,
        "local_m3u8_dir": local_m3u8_dir,
        "local_m3u8_path": os.path.join(local_m3u8_dir, f"{base_filename}.m3u8"),
        "s3_m3u8_dir": s3_m3u8_dir,
        "s3_m3u8_path": os.path.join(s3_m3u8_dir, f"{base_filename}.m3u8"),
      }
      mapped_courses.append({"course_id": course_id, "params": params})
    
    return mapped_courses

  ## Task #3 (Task Group)
  @task_group(group_id='video_processing_task_group')
  def video_processing_task_group(course_map):
    """
    Task group for video processing.

    Args:
        course_map (dict): A dictionary containing course information.
    """
    
    @task
    def get_mp4_from_s3(course):
      """
      Task to download MP4 file from S3.

      Args:
          course (dict): A dictionary containing course information.

      Raises:
          ValueError: If course_id or course_params are missing.
          AirflowException: If failed to download the file from S3.
      """
      course_id = course.get("course_id")
      course_params = course.get("params", {})
      logging.info(f"Runtime parameters for course #{course_id}: {json.dumps(course_params,indent=2)}")
      if not course_id or not course_params:
        raise ValueError("Missing course params")
      logging.info("<< Running s3 download task >>")
      raw_video_url = course_params["raw_video_url"]
      local_mp4_dir = course_params["local_mp4_dir"]
      local_mp4_path = course_params["local_mp4_path"]
      s3_path = f"s3://{s3_bucket}/{raw_video_url}"
      os.makedirs(local_mp4_dir, exist_ok=True)
      try:
        s3_exists = s3_hook.check_for_key(bucket_name=s3_bucket, key=raw_video_url)
        if not s3_exists:
          raise Exception(f"Key not found: {s3_path}")
        if os.path.exists(local_mp4_path):
          os.remove(local_mp4_path)
        logging.info(f"Downloading {s3_path} to {local_mp4_path}")
        file_name = s3_hook.download_file(
          bucket_name=s3_bucket,
          key=raw_video_url,
          local_path=local_mp4_dir,
          preserve_file_name=True,
          use_autogenerated_subdir=False,
        )
        logging.info(f"Successfully downloaded file: {file_name}")
      except Exception as e:
        raise AirflowException(f"Failed to download file from {s3_path}. Error message: {str(e)}")

    @task
    def run_ffmpeg(course):
      """
      Task to convert MP4 file to HLS format using ffmpeg.

      Args:
          course (dict): A dictionary containing course information.

      Raises:
          ValueError: If course_id or course_params are missing.
          AirflowException: If failed to execute ffmpeg command.
      """
      course_id = course.get("course_id")
      course_params = course.get("params", {})
      logging.info(f"Runtime parameters for course #{course_id}: {json.dumps(course_params,indent=2)}")
      if not course_id or not course_params:
        raise ValueError("Missing course params")
      logging.info("<< Running HLS conversion task >>")
      local_mp4_dir = course_params["local_mp4_dir"]
      local_mp4_path = course_params["local_mp4_path"]
      local_m3u8_dir = course_params["local_m3u8_dir"]
      local_m3u8_path = course_params["local_m3u8_path"]
      try:
        os.makedirs(local_m3u8_dir, exist_ok=True)
        logging.info(f"Running ffmpeg subprocess. Input file: {local_mp4_path}, Output file: {local_m3u8_path}")
        process = ffmpeg.input(
          local_mp4_path, # input mp4 file
        ).output(
          local_m3u8_path, # output m3u8 file
          format='hls',
          start_number=0,
          hls_time=10,
          hls_list_size=0,
        ).run_async(
          pipe_stdout=True,
          pipe_stderr=True
        )
        _, err = process.communicate()

        if not os.listdir(local_m3u8_dir):
          raise Exception(f"No files found in directory: '{local_m3u8_dir}/'")
        logging.info(f"Directory contents:")
        generated_files = sorted(
          [os.path.join(local_mp4_dir, gfile) for gfile in os.listdir(local_mp4_dir)] + \
          [os.path.join(local_m3u8_dir, gfile) for gfile in os.listdir(local_m3u8_dir)]
        )
        for gfile in generated_files:
          logging.info(f"File created: {gfile}")
      except Exception as e:
        raise AirflowException(f"Failed to execute ffmpeg command. Error message: {str(e)}. Stderr output: {err}")

    @task
    def upload_m3u8_to_s3(course):
      """
      Task to upload M3U8 file and associated TS files to S3.

      Args:
          course (dict): A dictionary containing course information.

      Raises:
          ValueError: If course_id or course_params are missing.
          AirflowException: If failed to upload files to S3.
      """
      course_id = course.get("course_id")
      course_params = course.get("params", {})
      logging.info(f"Runtime parameters for course #{course_id}: {json.dumps(course_params,indent=2)}")
      if not course_id or not course_params:
        raise ValueError("Missing course params")
      logging.info("<< Running S3 upload task >>")
      s3_m3u8_dir = course_params["s3_m3u8_dir"]
      s3_m3u8_path = course_params["s3_m3u8_path"]
      local_m3u8_dir = course_params["local_m3u8_dir"]
      local_m3u8_path = course_params["local_m3u8_path"]
      last_upload = None
      try:
        logging.info(f"Loading '{local_m3u8_path}' to 's3://{s3_bucket}/{s3_m3u8_path}'")
        s3_hook.load_file(
          filename=local_m3u8_path,
          key=s3_m3u8_path,
          bucket_name=s3_bucket,
          replace=True,
        )
        last_upload = local_m3u8_path
        for ts_file in os.listdir(local_m3u8_dir):
          if ts_file.endswith(".ts"):
            source = os.path.join(local_m3u8_dir, ts_file)
            dest = os.path.join(s3_m3u8_dir, ts_file)
            logging.info(f"Loading '{source}' to 's3://{s3_bucket}/{dest}'")
            s3_hook.load_file(
              filename=source,
              key=dest,
              bucket_name=s3_bucket,
              replace=True,
            )
            last_upload = source
      except Exception as e:
        raise AirflowException(f"Failed to upload file: {last_upload}. Error message: {str(e)}")

    def get_video_duration(local_m3u8_path):
      """
      Helper function to calculate the duration of the video from M3U8 file.

      Args:
          local_m3u8_path (str): Path to the local M3U8 file.

      Returns:
          int: Duration of the video in seconds.
      """
      logging.info("<< Running video duration task >>")
      duration = 0
      with open(local_m3u8_path, "r") as m3u8_file:
        for line in m3u8_file:
          if line.startswith("#EXTINF:"):
            duration += float(line.split(":")[1].split(",")[0])
      duration_seconds = round(duration)
      logging.info(f"Video duration = {duration_seconds} seconds")
      return duration_seconds

    @task
    def update_course_record(course):
      """
      Task to update course record in the database with video information.

      Args:
          course (dict): A dictionary containing course information.

      Raises:
          ValueError: If course_id or course_params are missing.
          AirflowException: If failed to update record in the database.
      """
      course_id = course.get("course_id")
      course_params = course.get("params", {})
      logging.info(f"Runtime parameters for course #{course_id}: {json.dumps(course_params,indent=2)}")
      if not course_id or not course_params:
        raise ValueError("Missing course params")
      logging.info("<< Running update database task >>")
      raw_video_url = course_params["raw_video_url"]
      local_m3u8_path = course_params["local_m3u8_path"]
      s3_m3u8_path = course_params["s3_m3u8_path"]
      try:
        if not os.path.exists(local_m3u8_path):
          raise ValueError(f"m3u8 file not found: {local_m3u8_path}")
        duration_seconds = get_video_duration(local_m3u8_path)
        logging.info(f"`duration_seconds` = {duration_seconds}, {type(duration_seconds)}")
        if not duration_seconds:
          raise Exception(f"Failed to retrieve video duration seconds")
        query = update_course_video_sql.format(
          s3_m3u8_path=s3_m3u8_path,
          duration_seconds=duration_seconds,
          timestamp=datetime.now(),
          course_id=course_id,
          raw_video_url=raw_video_url
        )
        logging.info(f"Query to execute: {query}")
        results = postgres_hook.run(sql=query)
        logging.info(f"Query results: {results}")
      except Exception as e:
        raise AirflowException(f"Failed to update record in postgres. Error message: {str(e)}")

    @task
    def clean_up(course):
      course_id = course.get("course_id")
      course_params = course.get("params", {})
      logging.info(f"Runtime parameters for course #{course_id}: {json.dumps(course_params,indent=2)}")
      if not course_id or not course_params:
        raise ValueError("Missing course id or course params")

      dirname = course_params["local_mp4_dir"]
      if os.path.exists(dirname):
        logging.info(f"*** Clearing out '{dirname}/' ***")
        for dirpath, dirnames, filenames in os.walk(dirname):
          logging.info(f"Searching dirnames: '{dirnames}'")
          for dirname in dirnames:
            dpath = os.path.join(dirpath, dirname)
            for filename in filenames:
              fpath = os.path.join(dpath, filename)
              if os.path.exists(fpath):
                os.remove(fpath)
                logging.info(f"File deleted: '{fpath}'")
            if os.path.exists(dpath):
              rmtree(dpath)
              logging.info(f"Directory deleted: '{dpath}'")
            else:
              logging.warning(f"Path does not exist and cannot be deleted: '{dpath}'")

      video_dir = os.path.join('include', 'datasets', 'videos')
      if os.path.exists(video_dir):
        video_dir_file_list = [os.path.join(video_dir, vfile) for vfile in os.listdir(video_dir)]
        logging.info(f"Remaining files found in '{video_dir}/': {', '.join(video_dir_file_list)}")

    ## Task Dependencies (Task Group)
    (
      get_mp4_from_s3(course_map) \
        >> run_ffmpeg(course_map) \
        >> [upload_m3u8_to_s3(course_map)] \
        >> update_course_record(course_map)
    ) >> clean_up(course_map)

  ## Task #4
  done = EmptyOperator(task_id="done", trigger_rule="all_done")
  
  ## Task Dependencies (DAG)
  mapped_courses = get_mapped_courses()
  process_video = video_processing_task_group.expand(course_map=mapped_courses)
  start >> mapped_courses >> process_video >> done

video_processing_dag_v2_instance = video_processing_dag_v2()
